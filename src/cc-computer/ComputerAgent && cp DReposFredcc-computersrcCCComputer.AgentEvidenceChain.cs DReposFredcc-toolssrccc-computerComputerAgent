using System.Diagnostics;
using System.Runtime.InteropServices;
using System.Text;
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;
using Microsoft.SemanticKernel.Connectors.OpenAI;
using Serilog;
using CCComputer.Agent.Plugins;
using CCComputer.Agent.Filters;
using CCComputer.Agent.Detection;

namespace CCComputer.Agent;

/// <summary>
/// Agent that controls the computer using desktop automation tools.
/// </summary>
public class ComputerControlAgent
{
    [DllImport("user32.dll")]
    private static extern IntPtr GetForegroundWindow();

    [DllImport("user32.dll", CharSet = CharSet.Auto, SetLastError = true)]
    private static extern int GetWindowText(IntPtr hWnd, StringBuilder lpString, int nMaxCount);

    [DllImport("user32.dll")]
    private static extern int GetWindowTextLength(IntPtr hWnd);

    private readonly Kernel _kernel;
    private readonly IChatCompletionService _chatService;
    private readonly ChatHistory _chatHistory;
    private readonly AgentConfig _config;
    private readonly ActivityLogger? _logger;
    private readonly DetectionPipeline? _detectionPipeline;

    /// <summary>
    /// Most recent detection result, available for element ID lookups.
    /// </summary>
    public DetectionResult? LastDetectionResult { get; private set; }

    /// <summary>
    /// Event raised when the agent is thinking/processing
    /// </summary>
    public event Action<string>? OnThinking;

    /// <summary>
    /// Event raised when a tool is being called
    /// </summary>
    public event Action<string, string?>? OnToolCall;

    /// <summary>
    /// Event raised when a tool returns a result
    /// </summary>
    public event Action<string, bool, string>? OnToolResult;

    /// <summary>
    /// Event raised when the agent has a response
    /// </summary>
    public event Action<string>? OnResponse;

    /// <summary>
    /// Event raised on error
    /// </summary>
    public event Action<string>? OnError;

    public ComputerControlAgent(AgentConfig config, ActivityLogger? logger = null)
    {
        Log.Debug("ComputerControlAgent constructor: ModelId={ModelId}, UserName={UserName}",
            config.ModelId, config.UserName);

        _config = config;
        _logger = logger;
        config.Validate();

        // Build the kernel with OpenAI
        var builder = Kernel.CreateBuilder();

        builder.AddOpenAIChatCompletion(
            modelId: config.ModelId,
            apiKey: config.ApiKey);

        _kernel = builder.Build();

        // Add tool logging filter
        var filter = new ToolLoggingFilter(
            logger: _logger,
            onToolCall: (name, args) => OnToolCall?.Invoke(name, args),
            onToolResult: (name, success, result) => OnToolResult?.Invoke(name, success, result));
        _kernel.FunctionInvocationFilters.Add(filter);

        // Get the chat completion service
        _chatService = _kernel.GetRequiredService<IChatCompletionService>();

        // Initialize chat history with system prompt
        _chatHistory = new ChatHistory();
        _chatHistory.AddSystemMessage(GetSystemPrompt());

        // Initialize multi-layered detection pipeline (UIA + OCR + PixelAnalysis)
        if (config.EnableDetectionPipeline)
        {
            try
            {
                _detectionPipeline = new DetectionPipeline(config.PythonPath, config.PixelDetectScriptPath);
                _detectionPipeline.EnableOcr = config.EnableOcrDetection;
                _detectionPipeline.EnablePixelAnalysis = config.EnablePixelAnalysis;
                Log.Information("Detection pipeline initialized (OCR={Ocr}, PixelAnalysis={Pixel})",
                    config.EnableOcrDetection, config.EnablePixelAnalysis);
            }
            catch (Exception ex)
            {
                Log.Warning(ex, "Failed to initialize detection pipeline — falling back to vision-only mode");
                _detectionPipeline = null;
            }
        }

        Log.Information("ComputerControlAgent initialized with model {ModelId}", config.ModelId);
    }

    /// <summary>
    /// Register the desktop automation plugins with the kernel
    /// </summary>
    public void RegisterPlugins()
    {
        Log.Debug("RegisterPlugins: registering plugins with cc_click path: {Path}", _config.CcClickPath);

        var desktopPlugin = new DesktopPlugin(_config.CcClickPath);
        var screenshotPlugin = new ScreenshotPlugin(_config.CcClickPath, _logger);  // Pass logger for session screenshots
        var shellPlugin = new ShellPlugin();
        // FileSystemPlugin disabled — agent must use GUI (action → verify → action)
        // var fileSystemPlugin = new FileSystemPlugin();
        var visionPlugin = new VisionPlugin(_config.ApiKey, _config.CcClickPath, _logger?.ScreenshotsFolder);

        _kernel.Plugins.AddFromObject(desktopPlugin, "Desktop");
        _kernel.Plugins.AddFromObject(screenshotPlugin, "Screenshot");
        _kernel.Plugins.AddFromObject(shellPlugin, "Shell");
        // _kernel.Plugins.AddFromObject(fileSystemPlugin, "FileSystem");
        _kernel.Plugins.AddFromObject(visionPlugin, "Vision");

        Log.Information("RegisterPlugins: registered 4 plugins (Desktop, Screenshot, Shell, Vision)");
    }

    /// <summary>
    /// Process a user request
    /// </summary>
    public async Task<string> ProcessRequestAsync(string userRequest, CancellationToken cancellationToken = default)
    {
        Log.Information("ProcessRequestAsync: starting request: {Request}",
            userRequest.Length > 100 ? userRequest[..100] + "..." : userRequest);

        _chatHistory.AddUserMessage(userRequest);
        _logger?.LogUserRequest(userRequest);

        var sw = Stopwatch.StartNew();
        const int maxIterations = 30;

        // Create evidence chain logger if we have a session folder
        var sessionId = _logger?.SessionFolder != null
            ? Path.GetFileName(_logger.SessionFolder)
            : DateTime.Now.ToString("yyyy-MM-dd_HHmmss");
        var sessionFolder = _logger?.SessionFolder ?? Path.GetTempPath();

        using var evidence = new EvidenceChainLogger(
            sessionFolder, sessionId, userRequest,
            _config.ModelId, _config.ScreenshotSettleDelayMs);

        try
        {
            OnThinking?.Invoke("Processing request...");
            _logger?.LogThinking("Processing request...");

            // Manual tool calling — we control the loop so we can inject screenshots
            var settings = new OpenAIPromptExecutionSettings
            {
                FunctionChoiceBehavior = FunctionChoiceBehavior.Auto(autoInvoke: false)
            };

            for (int iteration = 0; iteration < maxIterations; iteration++)
            {
                Log.Debug("ProcessRequestAsync: iteration {Iteration}, calling LLM", iteration);

                var result = await _chatService.GetChatMessageContentAsync(
                    _chatHistory,
                    settings,
                    _kernel,
                    cancellationToken);

                // Record the LLM's observation on the previous step (if any).
                // The LLM is responding to the screenshot it saw, so its text content
                // is the observation for the step that just completed.
                if (iteration > 0 && !string.IsNullOrEmpty(result.Content))
                {
                    evidence.RecordObservation(result.Content);
                }

                // Add assistant response to history
                _chatHistory.Add(result);

                // Check for function calls
                var functionCalls = result.Items.OfType<FunctionCallContent>().ToList();

                if (functionCalls.Count == 0)
                {
                    // No tool calls — this is the final text response
                    sw.Stop();
                    var responseText = result.Content ?? "I completed the task.";

                    Log.Information("ProcessRequestAsync: completed in {ElapsedMs}ms after {Iterations} iterations, response length: {ResponseLength}",
                        sw.ElapsedMilliseconds, iteration, responseText.Length);

                    _logger?.LogResponse(responseText);
                    _logger?.LogTaskComplete(true, responseText);
                    evidence.CompleteSuccess(responseText);
                    OnResponse?.Invoke(responseText);
                    return responseText;
                }

                // Begin a new evidence step before executing tools
                evidence.BeginStep();

                // Execute each function call with evidence tracking
                foreach (var fc in functionCalls)
                {
                    var toolName = $"{fc.PluginName}.{fc.FunctionName}";
                    var args = fc.Arguments?.ToDictionary(
                        kvp => kvp.Key,
                        kvp => kvp.Value) ?? [];

                    evidence.RecordActionStart(toolName, args);

                    FunctionResultContent resultContent;
                    bool success;
                    string resultText;
                    string? errorMsg = null;

                    try
                    {
                        resultContent = await fc.InvokeAsync(_kernel, cancellationToken);
                        resultText = resultContent.Result?.ToString() ?? "(no result)";
                        success = true;
                    }
                    catch (Exception ex)
                    {
                        Log.Warning(ex, "Tool {Plugin}.{Function} failed", fc.PluginName, fc.FunctionName);
                        resultContent = new FunctionResultContent(fc, ex);
                        resultText = $"Error: {ex.Message}";
                        success = false;
                        errorMsg = ex.Message;
                    }

                    evidence.RecordActionComplete(resultText, success, errorMsg);
                    _chatHistory.Add(resultContent.ToChatMessage());
                }

                // Wait for screen to settle before capturing screenshot
                if (_config.ScreenshotSettleDelayMs > 0)
                {
                    await Task.Delay(_config.ScreenshotSettleDelayMs, cancellationToken);
                }

                // Screenshot-in-the-loop: capture the foreground window (fall back to full desktop)
                var foregroundTitle = GetForegroundWindowTitle();
                var (screenshotBytes, sessionPath, sizeBytes, screenshotTempPath) = await CaptureScreenshotBytesAsync(foregroundTitle);
                // Fall back to full desktop if window capture failed
                if (screenshotBytes == null && foregroundTitle != null)
                {
                    Log.Debug("Window screenshot failed for '{Window}', falling back to full desktop", foregroundTitle);
                    (screenshotBytes, sessionPath, sizeBytes, screenshotTempPath) = await CaptureScreenshotBytesAsync();
                    foregroundTitle = null;
                }
                if (screenshotBytes != null)
                {
                    // Record screenshot in evidence chain
                    if (sessionPath != null)
                    {
                        evidence.RecordScreenshot(sessionPath, sizeBytes);
                    }

                    // Run multi-layered detection pipeline if enabled
                    byte[] imageForLlm = screenshotBytes;
                    string elementSummary = "";

                    if (_detectionPipeline != null && foregroundTitle != null)
                    {
                        try
                        {
                            var (elements, annotatedBytes) = await _detectionPipeline.DetectFromBytesAsync(
                                foregroundTitle, screenshotBytes);

                            if (elements.Count > 0)
                            {
                                LastDetectionResult = new DetectionResult
                                {
                                    WindowTitle = foregroundTitle,
                                    Elements = elements,
                                };

                                // Use annotated screenshot (with numbered bounding boxes)
                                if (annotatedBytes != null)
                                {
                                    imageForLlm = annotatedBytes;
                                }

                                // Generate element summary text
                                elementSummary = "\n\n" + AnnotatedScreenshotRenderer.GenerateElementSummary(elements);
                                Log.Debug("DetectionPipeline: {Count} elements detected for '{Window}'",
                                    elements.Count, foregroundTitle);
                            }
                        }
                        catch (Exception ex)
                        {
                            Log.Warning(ex, "Detection pipeline failed — using raw screenshot");
                        }
                    }

                    var screenshotLabel = foregroundTitle != null
                        ? $"[Screenshot of '{foregroundTitle}' window after your action]"
                        : "[Screenshot of desktop after your action]";

                    if (!string.IsNullOrEmpty(elementSummary))
                    {
                        screenshotLabel += "\n\nThe screenshot has numbered bounding boxes overlaid. " +
                            "Use the element numbers below to reference elements when clicking." +
                            elementSummary;
                    }

                    var items = new ChatMessageContentItemCollection
                    {
                        new TextContent(screenshotLabel),
                        new ImageContent(imageForLlm, "image/png")
                    };
                    _chatHistory.AddUserMessage(items);
                    Log.Debug("ProcessRequestAsync: injected screenshot after iteration {Iteration} (window: {Window})", iteration, foregroundTitle ?? "desktop");
                }

                // Periodic save for crash recovery
                evidence.Save();

                OnThinking?.Invoke($"Thinking... (step {iteration + 1})");
            }

            // Hit max iterations
            sw.Stop();
            var maxIterResponse = "I've reached the maximum number of steps. Please check the screen and let me know if you'd like me to continue.";

            Log.Warning("ProcessRequestAsync: hit max iterations ({Max})", maxIterations);

            _logger?.LogResponse(maxIterResponse);
            _logger?.LogTaskComplete(false, "Hit max iterations");
            evidence.CompleteMaxIterations(maxIterResponse);
            OnResponse?.Invoke(maxIterResponse);
            return maxIterResponse;
        }
        catch (OperationCanceledException)
        {
            sw.Stop();
            Log.Information("ProcessRequestAsync: cancelled by user after {ElapsedMs}ms", sw.ElapsedMilliseconds);
            _logger?.LogStopped("Operation cancelled by user");
            evidence.CompleteCancelled();
            throw;
        }
        catch (Exception ex)
        {
            sw.Stop();
            Log.Error(ex, "ProcessRequestAsync: failed after {ElapsedMs}ms", sw.ElapsedMilliseconds);
            _logger?.LogError($"Error: {ex.Message}", ex);
            evidence.CompleteError(ex.Message);
            OnError?.Invoke($"Error: {ex.Message}");
            throw;
        }
    }

    /// <summary>
    /// Simple chat without tools (for testing LLM connection)
    /// </summary>
    public async Task<string> ChatAsync(string message, CancellationToken cancellationToken = default)
    {
        _chatHistory.AddUserMessage(message);

        try
        {
            var response = await _chatService.GetChatMessageContentAsync(
                _chatHistory,
                cancellationToken: cancellationToken);

            var responseText = response.Content ?? "";
            _chatHistory.AddAssistantMessage(responseText);

            return responseText;
        }
        catch (Exception ex)
        {
            OnError?.Invoke($"Chat error: {ex.Message}");
            throw;
        }
    }

    /// <summary>
    /// Clear the chat history (keep system prompt)
    /// </summary>
    public void ClearHistory()
    {
        var systemPrompt = _chatHistory.FirstOrDefault(m => m.Role == AuthorRole.System);
        _chatHistory.Clear();
        if (systemPrompt != null)
        {
            _chatHistory.Add(systemPrompt);
        }
    }

    /// <summary>
    /// Get the title of the current foreground window, or null if none.
    /// </summary>
    private static string? GetForegroundWindowTitle()
    {
        try
        {
            var hWnd = GetForegroundWindow();
            if (hWnd == IntPtr.Zero) return null;

            int length = GetWindowTextLength(hWnd);
            if (length == 0) return null;

            var sb = new StringBuilder(length + 1);
            GetWindowText(hWnd, sb, sb.Capacity);
            var title = sb.ToString();
            return string.IsNullOrWhiteSpace(title) ? null : title;
        }
        catch
        {
            return null;
        }
    }

    /// <summary>
    /// Capture a screenshot of a specific window (or full desktop if no window specified).
    /// Returns the PNG bytes, session-relative path, size, and temp file path.
    /// Used by the screenshot-in-the-loop pattern to inject vision after every action.
    /// </summary>
    private async Task<(byte[]? Bytes, string? SessionPath, long SizeBytes, string? TempPath)> CaptureScreenshotBytesAsync(string? windowTitle = null)
    {
        var tempPath = Path.Combine(Path.GetTempPath(), $"cc_loop_{DateTime.Now:yyyyMMdd_HHmmss_fff}.png");

        try
        {
            var args = $"screenshot -o \"{tempPath}\"";
            if (!string.IsNullOrEmpty(windowTitle))
            {
                args += $" -w \"{windowTitle}\"";
            }

            var psi = new ProcessStartInfo
            {
                FileName = _config.CcClickPath,
                Arguments = args,
                UseShellExecute = false,
                RedirectStandardOutput = true,
                RedirectStandardError = true,
                CreateNoWindow = true
            };

            using var process = new Process { StartInfo = psi };
            process.Start();

            await process.StandardOutput.ReadToEndAsync();
            await process.StandardError.ReadToEndAsync();
            await process.WaitForExitAsync();

            if (process.ExitCode != 0 || !File.Exists(tempPath))
            {
                Log.Warning("Screenshot capture failed (exit code {ExitCode})", process.ExitCode);
                return (null, null, 0, null);
            }

            var bytes = await File.ReadAllBytesAsync(tempPath);

            // Log to session folder if available — returns the session-relative path
            string? sessionPath = _logger?.LogScreenshot(tempPath, "Auto-capture after action");

            return (bytes, sessionPath, bytes.LongLength, tempPath);
        }
        catch (Exception ex)
        {
            Log.Warning(ex, "Failed to capture loop screenshot");
            return (null, null, 0, null);
        }
        finally
        {
            try { if (File.Exists(tempPath)) File.Delete(tempPath); } catch { }
        }
    }

    private string GetSystemPrompt()
    {
        return $"""
            You are CC Computer, an AI agent that can control a Windows computer to help {_config.UserName} accomplish tasks.

            ## Your Capabilities
            You have access to tools that let you:
            - List all open windows
            - List UI elements in a window (buttons, text fields, menus, etc.)
            - Click on UI elements by name or coordinates
            - Type text into focused fields
            - Send keyboard shortcuts (Enter, Tab, Ctrl+C, etc.)
            - Execute keyboard shortcuts (keyboard_shortcut) - MORE RELIABLE than clicking menus
            - Focus windows to bring them to the foreground
            - Launch applications (Notepad, Outlook, Excel, Chrome, etc.)
            - Minimize all windows for a clean desktop
            - Position windows on the left or right half of the screen

            ## DESCRIBE BEFORE ACTING (EVIDENCE CHAIN)
            Before issuing any tool call, you MUST first describe what you observe in the
            screenshot. Start with: "I can see [what's on screen]. I will now [action]."
            This is recorded in the evidence chain for auditing. Every response with tool
            calls MUST include observation text.

            ## HOW YOU SEE THE SCREEN
            After every action you take, a screenshot of the desktop will be automatically
            captured and shown to you. The screenshot has numbered bounding boxes overlaid
            on detected UI elements, and a structured element list is provided alongside it.

            USE THE ELEMENT NUMBERS to reference UI elements when clicking. For example:
            - If element [3] is "Save" Button at (150,300), use click(coordinates: "150,300")
            - The coordinates in the element list are pixel-perfect — trust them over your
              own coordinate estimates from the screenshot.

            The detection system uses Windows UI Automation (pixel-perfect) + OCR (for
            custom-drawn text) to find elements. Element coordinates are far more accurate
            than vision-model coordinate guessing.

            You also have vision tools available as a fallback if you need a targeted description:
            - vision_describe(): Gets a text description of the current screen
            - vision_verify("condition"): Checks whether a condition is true on screen

            But normally, use the element list + annotated screenshot provided after actions.

            ## ONE STEP AT A TIME (CRITICAL)

            Perform ONE action at a time, then LOOK at the screenshot that follows.

            1. ACT: Perform exactly ONE action (click, type, shortcut, etc.)
            2. LOOK: A screenshot will appear — examine it to see what happened
            3. DECIDE: If the action succeeded, proceed. If not, reassess.

            NEVER chain multiple actions without looking at the screenshot between them.

            Examples of correct behavior:
            - Send Ctrl+Shift+S → look at screenshot → see Save As dialog → type path
            - Type text → look at screenshot → see text in editor → proceed to save
            - Launch Notepad → look at screenshot → see Notepad is ready → start typing

            ## KEYBOARD SHORTCUTS - PREFER OVER MENUS
            When UI automation fails (list_elements returns errors, modern UWP apps), use keyboard shortcuts:
            - Save As: Ctrl+Shift+S (works in Notepad, Word, most apps)
            - New: Ctrl+N
            - Save: Ctrl+S
            - Open: Ctrl+O
            - Close: Ctrl+W or Alt+F4
            - Copy/Paste: Ctrl+C, Ctrl+V

            Use keyboard_shortcut tool BEFORE trying to click menu items - it's faster and more reliable!
            Example: Instead of clicking File -> Save As, use keyboard_shortcut("Ctrl+Shift+S", "Notepad")

            ## CLICKING ELEMENTS — USE DETECTED ELEMENT COORDINATES
            After each action, you receive a list of detected elements with IDs and coordinates.
            ALWAYS prefer using the detected element coordinates for clicking:

            ORDER OF PREFERENCE for clicking/interacting:
            1. click(coordinates: "x,y") using coordinates from the detected element list (most accurate)
            2. keyboard_shortcut (fastest, most reliable for menu operations)
            3. click with elementName from list_elements (legacy method)
            4. vision_click (slowest fallback, uses separate vision API call)

            ## VERIFICATION

            After every action, a screenshot is shown to you. Use it to verify success:
            - Did the app open? Is the dialog visible? Did the text appear?
            - If the screenshot is unclear, use vision_describe() for more detail

            NEVER claim success without looking at the screenshot first!

            ## HONEST REPORTING (CRITICAL)

            If an action fails:
            - Say "FAILED: [reason]"
            - Do NOT say "I have successfully..." when you haven't
            - Describe what you see in the screenshot
            - Suggest what went wrong and alternatives

            The user will review your session screenshots. Be honest about what happened.
            Every session is logged with screenshots - lying about success will be discovered.

            If you cannot complete a task:
            - Report exactly what you tried
            - Report exactly what failed
            - Suggest alternative approaches the user could try

            ## CRITICAL: Robustness Rules
            Actions can fail if windows lose focus or are occluded. Follow these rules:
            1. Before clicking or typing: use get_foreground_window to verify the target window has focus
            2. If the wrong window has focus: use focus_window to bring the target window to front, then retry
            3. If an action fails: re-focus the window and retry (up to 3 times before giving up)
            4. When opening an application: use position_window to place it on the LEFT side of the screen
               (CC Computer runs on the right side so user can watch and hit STOP if needed)
            5. After any UI action: look at the screenshot to confirm the expected result occurred

            ## How to Accomplish Tasks
            1. Look at the initial screenshot to see the current desktop state
            2. Plan what needs to be done
            3. When opening an app, position it on the LEFT side of the screen
            4. PREFER keyboard shortcuts over clicking menus (more reliable on modern apps)
            5. Before each action, verify the correct window has focus
            6. Execute ONE action at a time, then look at the screenshot
            7. If action fails, re-focus and retry up to 3 times
            8. Continue until the task is complete and confirmed by screenshot

            ## Important Guidelines
            - Be methodical — look at the screenshot after each action to verify it worked
            - If list_elements fails on an app, try keyboard shortcuts instead
            - For typing into fields: first click the field, then use send_keys
            - Use keyboard_shortcut for reliable menu operations
            - When opening apps, position them on the LEFT so the user can see both the app and CC Computer

            ## HANDLING UNEXPECTED DIALOGS
            Windows frequently shows confirmation dialogs. When you see one in the screenshot:

            1. READ: Understand what the dialog is asking (file exists, confirm delete, etc.)
            2. DECIDE: Choose the appropriate response based on the task goal
            3. ACT: Click the appropriate button ("Yes", "No", "OK", "Cancel")
            4. LOOK: Check the next screenshot to confirm the dialog was dismissed

            Common dialogs and how to handle them:
            - "File already exists. Replace?" -> Click "Yes" to overwrite, or "No" and use a different filename
            - "Save changes?" -> Click "Yes" if you want to save, "No" to discard, "Cancel" to go back
            - "Are you sure?" -> Click "Yes" if the action is intentional
            - "Do you want to save?" -> Click "Save" or "Don't Save" based on task requirements

            ## Current User
            You are helping {_config.UserName}.
            """;
    }
}
